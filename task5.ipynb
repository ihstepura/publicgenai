{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVTyi9WV9DS6TpDQ/dSyMR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ihstepura/publicgenai/blob/main/task5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8de0_TBbNR4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "940673b1"
      },
      "source": [
        "# Task\n",
        "Build a Gradio semantic search tool for the \"financial_news.csv\" dataset. This tool should allow users to input a search query and retrieve the top 5 most semantically similar news articles from the dataset. The tool should also extract and display any URLs present in the news text, while removing them from the text used for embedding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ab6432f"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "### Subtask:\n",
        "Load the 'financial_news.csv' file into a pandas DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bb82041"
      },
      "source": [
        "**Reasoning**:\n",
        "To load the data, I will first import the pandas library, then read the 'financial_news.csv' file into a DataFrame, display its first 5 rows, and print the column names to verify the loading process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "127fc7ea",
        "outputId": "36849750-573a-4422-bfe2-e8a03ffe9ad0"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/financial_news.csv')\n",
        "print(\"First 5 rows of the DataFrame:\")\n",
        "print(df.head())\n",
        "print(\"\\nColumn names of the DataFrame:\")\n",
        "print(df.columns)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of the DataFrame:\n",
            "                                                text  label\n",
            "0  Here are Thursday's biggest analyst calls: App...      0\n",
            "1  Buy Las Vegas Sands as travel to Singapore bui...      0\n",
            "2  Piper Sandler downgrades DocuSign to sell, cit...      0\n",
            "3  Analysts react to Tesla's latest earnings, bre...      0\n",
            "4  Netflix and its peers are set for a ‘return to...      0\n",
            "\n",
            "Column names of the DataFrame:\n",
            "Index(['text', 'label'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "234da522"
      },
      "source": [
        "## Extract and Clean URLs\n",
        "\n",
        "### Subtask:\n",
        "Iterate through the 'text' column to identify and extract URLs. Store these URLs in a new 'URL' column and remove them from the original 'text' column.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b49a775b"
      },
      "source": [
        "**Reasoning**:\n",
        "To extract and remove URLs from the 'text' column, I need to use regular expressions. I will define functions for extraction and removal, and then apply them to the DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc81dcce",
        "outputId": "70b5836c-35d1-4013-b68c-d3973f13c4c8"
      },
      "source": [
        "import re\n",
        "\n",
        "# Function to extract URLs\n",
        "def extract_urls(text):\n",
        "    # Regex to find URLs (http/https followed by non-whitespace characters)\n",
        "    url_pattern = re.compile(r'https?://\\S+')\n",
        "    urls = url_pattern.findall(text)\n",
        "    return urls if urls else []\n",
        "\n",
        "# Function to remove URLs\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+')\n",
        "    return url_pattern.sub('', text).strip()\n",
        "\n",
        "# Apply the extract_urls function to create the 'urls' column\n",
        "df['urls'] = df['text'].apply(extract_urls)\n",
        "\n",
        "# Apply the remove_urls function to clean the 'text' column\n",
        "df['text'] = df['text'].apply(remove_urls)\n",
        "\n",
        "print(\"First 5 rows of the DataFrame after URL extraction and removal:\")\n",
        "print(df.head())\n",
        "print(\"\\nFirst 5 entries of the 'urls' column:\")\n",
        "print(df['urls'].head())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of the DataFrame after URL extraction and removal:\n",
            "                                                text  label urls\n",
            "0  Here are Thursday's biggest analyst calls: App...      0   []\n",
            "1  Buy Las Vegas Sands as travel to Singapore bui...      0   []\n",
            "2  Piper Sandler downgrades DocuSign to sell, cit...      0   []\n",
            "3  Analysts react to Tesla's latest earnings, bre...      0   []\n",
            "4  Netflix and its peers are set for a ‘return to...      0   []\n",
            "\n",
            "First 5 entries of the 'urls' column:\n",
            "0    []\n",
            "1    []\n",
            "2    []\n",
            "3    []\n",
            "4    []\n",
            "Name: urls, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7b9beb7"
      },
      "source": [
        "## Generate Sentence Embeddings\n",
        "\n",
        "### Subtask:\n",
        "Generate sentence embeddings for the cleaned 'text' column.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5535c8f1"
      },
      "source": [
        "**Reasoning**:\n",
        "To generate sentence embeddings, the `sentence-transformers` library is required. This step ensures it is installed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41ca04b1",
        "outputId": "ed8a8af2-dc2c-410d-c35d-d14797719c6d"
      },
      "source": [
        "pip install sentence-transformers"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.1)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2026.1.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb18fb8c"
      },
      "source": [
        "**Reasoning**:\n",
        "To generate sentence embeddings, I will import the necessary library, load a pre-trained model, and then encode the 'text' column, storing the results in a new 'embeddings' column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3a55d9f",
        "outputId": "cbbf757e-e930-4698-b276-01bf5cfba9ec"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load a pre-trained sentence transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Generate sentence embeddings for the 'text' column\n",
        "df['embeddings'] = df['text'].apply(lambda x: model.encode(x))\n",
        "\n",
        "print(\"First 5 entries of the 'embeddings' column (showing shape):\")\n",
        "print(df['embeddings'].head().apply(lambda x: x.shape))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 entries of the 'embeddings' column (showing shape):\n",
            "0    (384,)\n",
            "1    (384,)\n",
            "2    (384,)\n",
            "3    (384,)\n",
            "4    (384,)\n",
            "Name: embeddings, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd252680"
      },
      "source": [
        "## Build Gradio Semantic Search App\n",
        "\n",
        "### Subtask:\n",
        "Create a Gradio interface that allows users to input search queries. Implement a function to compute cosine similarity between the query embedding and the pre-computed text embeddings, then return the top 5 closest records from the DataFrame. Display these results in the Gradio app.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ed4b212"
      },
      "source": [
        "**Reasoning**:\n",
        "To build the Gradio application, the `gradio` library needs to be installed first. This step ensures that the necessary library is available in the environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2f71d1b",
        "outputId": "e5c0acb4-e9ed-40e1-f104-c2dbe96f1ccf"
      },
      "source": [
        "pip install gradio"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.12.1)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.123.10)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.22)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.14)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.50.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.40.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f0010be"
      },
      "source": [
        "**Reasoning**:\n",
        "To implement the semantic search functionality and display it using Gradio, I need to define the core search logic, including embedding the query, calculating cosine similarity, and retrieving the top results. Then, I will set up the Gradio interface to interact with this function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "6c2df199",
        "outputId": "c5056acf-424e-4785-eb7e-6cded03832e8"
      },
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Ensure the model is loaded (it was loaded in a previous step, but good to ensure context)\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Convert embeddings to a 2D numpy array for efficient computation\n",
        "doc_embeddings = np.array(df['embeddings'].tolist())\n",
        "\n",
        "def semantic_search(query):\n",
        "    # Encode the query\n",
        "    query_embedding = model.encode(query)\n",
        "\n",
        "    # Calculate cosine similarity between query and document embeddings\n",
        "    # Reshape query_embedding to (1, -1) for cosine_similarity function if it's 1D\n",
        "    similarities = cosine_similarity(query_embedding.reshape(1, -1), doc_embeddings)[0]\n",
        "\n",
        "    # Get the indices of the top 5 most similar articles\n",
        "    top_indices = np.argsort(similarities)[::-1][:5]\n",
        "\n",
        "    # Retrieve the corresponding articles and their URLs\n",
        "    results = []\n",
        "    for i in top_indices:\n",
        "        text_entry = df.iloc[i]['text']\n",
        "        urls_entry = df.iloc[i]['urls']\n",
        "        label_entry = df.iloc[i]['label']\n",
        "        results.append({\"Text\": text_entry, \"Label\": label_entry, \"URLs\": urls_entry})\n",
        "\n",
        "    # Convert list of dicts to a format suitable for gr.Dataframe\n",
        "    if results:\n",
        "        output_df = pd.DataFrame(results)\n",
        "    else:\n",
        "        output_df = pd.DataFrame(columns=[\"Text\", \"Label\", \"URLs\"])\n",
        "\n",
        "    return output_df\n",
        "\n",
        "# Create the Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=semantic_search,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your search query here...\"),\n",
        "    outputs=gr.Dataframe(headers=[\"Text\", \"Label\", \"URLs\"], row_count=5),\n",
        "    title=\"Financial News Semantic Search\",\n",
        "    description=\"Enter a query to find the top 5 most semantically similar financial news articles. URLs present in the original text are extracted and displayed separately.\"\n",
        ")\n",
        "\n",
        "# Launch the app\n",
        "iface.launch()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://234fac2fc2e46c8370.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://234fac2fc2e46c8370.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34ffa757"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the functionality of the semantic search tool and provide instructions on how to use it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d6baabc"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "*   **What is the functionality of the semantic search tool?**\n",
        "    The semantic search tool allows users to input a text query and retrieves the top 5 most semantically similar financial news articles from the `financial_news.csv` dataset. It utilizes sentence embeddings and cosine similarity for matching. Additionally, it extracts any URLs present in the original news text, removes them from the text used for embedding, and displays these extracted URLs alongside the search results.\n",
        "\n",
        "*   **How do I use the semantic search tool?**\n",
        "    1.  Access the launched Gradio application via the provided public URL.\n",
        "    2.  Enter your search query into the designated text box.\n",
        "    3.  The application will then display a table containing the \"Text,\" \"Label,\" and \"URLs\" of the top 5 most semantically similar financial news articles.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The `financial_news.csv` dataset was successfully loaded, containing 'text' and 'label' columns.\n",
        "*   URLs were successfully identified and extracted from the 'text' column using a regular expression (`https?://\\S+`). These URLs were stored in a new 'urls' column, and the 'text' column was cleaned by removing them.\n",
        "*   Sentence embeddings for the cleaned 'text' column were generated using the 'all-MiniLM-L6-v2' Sentence Transformer model, resulting in 384-dimensional vectors for each news article.\n",
        "*   A Gradio application was developed to facilitate semantic search, employing cosine similarity to rank articles based on their semantic relevance to the user's query.\n",
        "*   The Gradio interface effectively displays the top 5 search results in a structured `gr.Dataframe`, showing the cleaned text, its associated label, and any extracted URLs.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The developed semantic search tool offers a more intuitive and context-aware way to find financial news compared to traditional keyword-based searches, enabling users to discover relevant articles even with different phrasing of their queries.\n",
        "*   To enhance the tool's capabilities, consider implementing a mechanism for users to provide feedback on search result relevance, which could be used to fine-tune the embedding model or improve the ranking algorithm over time.\n"
      ]
    }
  ]
}